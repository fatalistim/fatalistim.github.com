---
layout: post
title: 使用wordpress robots.txt文件禁止google抓取博客重复内容
date: 2010-06-18 20:55
tags: [robots.txt作用, robots.txt写作方法, robots.txt写法, robots.txt怎么写, robots.txt格式, robots.txt用法, wordpress, 电脑网络]
---
一、昨天修改了博客的wordpress robots.txt文件，这样修改之后可以：

1.禁止google抓取博客的重复内容

2.提升搜索引擎蜘蛛抓取博客网页链接的效率以提高索引页面

3.提升博客权重

4.robots.txt作用是禁止google robots抓取一些网站上的目录和内容

二、robots.txt格式、robots.txt写作方法、robots.txt用法、robots.txt怎么写？

我的wordpress博客robots.txt写法：

User-agent: *
Disallow: /wap/
Disallow: /page/
Disallow: /feed/
Disallow: /blog/author/admin/
Disallow: /blog/2008/
Disallow: /blog/2009/
Disallow: /blog/2010/
Disallow: /blog/tags/*/page/
Sitemap: <a href="http://i.lvshiminglu.com/sitemap.xml.gz" target="_self">http://i.lvshiminglu.com/sitemap.xml.gz</a>

1.找出wordpress博客重复内容的方法：site:fatalist.im/wap OR page OR feed OR author/admin OR tags

2./wap/ wordpress博客手机版本的地址，内容和html版本内容严重重复，不利用搜索引擎优化。

3./page/ wordpress博客翻页内容，页面标题完全重复，这些内容几乎不能从搜索引擎带来什么流量。

4./blog/author/admin/ wordpress博客作者页面内容，同page页面，没有在搜索结果中存在的必要。

5./blog/2008/ 日期页面。

6./blog/tags/*/page/ 文章分类翻页页面。

三、等待后续效果：

昨天已经提交robots.txt文件，google webmasters tools 后台也已经更新robots.txt文件内容，不过之前收录的重复内容还在，不知道过段时间会不会从索引的数据里删除，还是从加上robots.txt文件后不再抓取之后的内容。

<a href="http://i.lvshiminglu.com/">尘世客博客</a>：<a href="http://i.lvshiminglu.com/">http://i.lvshiminglu.com/</a>

